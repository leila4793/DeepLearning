{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23739b31-598b-41bb-aac6-fae49009d76d",
   "metadata": {},
   "source": [
    "__content:__\n",
    "1. [Autograd in PyTorch](#AutogradPyTorch)\n",
    "2. [Using Autograd for Polynomial Regression](#AutogradPolynomialRegression)\n",
    "3. [Using Autograd to Solve a Math Puzzle](#AutogradSolvePuzzle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecfcfbb-6f95-485a-a9f9-44bbb23146d9",
   "metadata": {},
   "source": [
    "# <h2 style=\"color: blue;\"> 1.Autograd in PyTorch <a id='AutogradPyTorch'></a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13d8a7-2719-438c-b806-c798d61cf4a5",
   "metadata": {},
   "source": [
    "<font color='#808080'>\n",
    "    \n",
    "In PyTorch, you can create tensors as variables or constants and build an expression with\n",
    "them. The expression is essentially a function of the variable tensors. Therefore, you may\n",
    "derive its derivative function, i.e., the differentiation or the gradient. In PyTorch, you can create a __constant matrix__ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45766aa-79da-419a-9799-7ae9df0e7fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "torch.Size([3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ddc67-a726-47e8-b934-9e4fa006d29d",
   "metadata": {},
   "source": [
    "<font color='#808080'>\n",
    "\n",
    "this tensor is not assumed to be a variable for a function in the sense that differentiation\n",
    "with it is not supported. You can create tensors that work like a __variable__ with an extra option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7b7c79-b970-4130-8536-65693a948fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "torch.Size([3])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad = True)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe4fdb6-135a-4a4d-968b-428c999aadd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor(3.6000, requires_grad=True)\n",
      "y = tensor(12.9600, grad_fn=<MulBackward0>)\n",
      "x.grad = tensor(7.2000)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.6, requires_grad = True)\n",
    "y = x * x\n",
    "y.backward()\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"x.grad =\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e991bd-77ec-4ef9-9fba-0d4882e26795",
   "metadata": {},
   "source": [
    "<font color='#808080'>\n",
    "\n",
    "What it does is the following: This defined a variable $x$ (with value $3.6$) and then computed\n",
    "$y = x \\times x$ or $y = x ^ {2}$. Then you ask for the differentiation of $y$. Since $y$ obtained its value from $x$, you can find the derivative $\\frac {dy}{dx}$ at __x.grad__, in the form of a tensor, immediately after you run __y.backward()__. You know $y = x^{2}$ means $y^{'} = 2x$. Hence the output would give you a value of $3.6 \\times 2 = 7.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842eab62-8ec8-4bf1-9024-a1426b5c2ac1",
   "metadata": {},
   "source": [
    "# <h2 style=\"color: blue;\"> 2.Using Autograd for Polynomial Regression <a id='AutogradPolynomialRegression'></a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81320ad9-f847-4804-b11c-937f9566b47a",
   "metadata": {},
   "source": [
    "<font color='#808080'>\n",
    "\n",
    "Letâ€™s consider an example. You can build a polynomial $f(x) = x^{2} + 2x + 3$ in __NumPy__ as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220a19f0-d4dc-4688-a9bc-01c571d5281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2\n",
      "1 x + 2 x + 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "polynomial = np.poly1d([1, 2, 3])\n",
    "print(polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4cd61d-f7f2-4383-91b5-192b2b429cc0",
   "metadata": {},
   "source": [
    "<font color='#808080'>\n",
    "\n",
    "You may use the polynomial as a function, such as:\n",
    "```python\n",
    "print(polynomial(1.5))\n",
    "```\n",
    "And this prints $8.25$, for $(1.5)^{2} + 2 \\times (1.5) + 3 = 8.25$.\n",
    "Now you can generate a number of samples from this function using __NumPy__:(remember that both $X$ and $Y$ are NumPy arrays of the shape $(20,1)$, and they are related as $y = f(x)$ for the polynomial $f(x)$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095f9949-cb1f-4a47-a3e7-b94c7faae8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.25\n",
      "[[  2.19214872]\n",
      " [ 13.96736855]\n",
      " [ 25.16547915]\n",
      " [  3.83473191]\n",
      " [ 35.09627477]\n",
      " [  2.03643278]\n",
      " [  3.56013023]\n",
      " [ 85.10614604]\n",
      " [  3.85798294]\n",
      " [118.68877768]\n",
      " [151.35571859]\n",
      " [  3.19670162]\n",
      " [  5.01118283]\n",
      " [  7.27846113]\n",
      " [  3.32798729]\n",
      " [  8.4853999 ]\n",
      " [ 13.79119342]\n",
      " [  8.55030693]\n",
      " [ 16.8655217 ]\n",
      " [  4.38718173]]\n"
     ]
    }
   ],
   "source": [
    "print(polynomial(1.5))\n",
    "N = 20 # number of samples\n",
    "# Generate random samples roughly between -10 to +10\n",
    "X = np.random.randn(N,1) * 5\n",
    "Y = polynomial(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624994c3-87d3-40b4-81bd-729438c68918",
   "metadata": {},
   "source": [
    "<font color='#808080'>\n",
    "\n",
    "Now, assume you do not know what the polynomial is except it is quadratic. And you\n",
    "want to recover the coefficients. Since a quadratic polynomial is in the form of $Ax^{2} + Bx + C$, you have three unknowns to find. You can find them using the gradient descent algorithm\n",
    "you implement or an existing gradient descent optimizer. The following demonstrates how it\n",
    "works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3ebba0-58ba-42b7-845d-8483430cf3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1114],\n",
      "        [-1.0434],\n",
      "        [-0.5185]], requires_grad=True)\n",
      "tensor([[1.0111],\n",
      "        [1.9969],\n",
      "        [2.0004]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Assume samples X and Y are prepared elsewhere\n",
    "XX = np.hstack([X*X, X, np.ones_like(X)])\n",
    "w = torch.randn(3, 1, requires_grad = True) # the 3 coefficients\n",
    "x = torch.tensor(XX, dtype = torch.float32) # input sample\n",
    "y = torch.tensor(Y, dtype = torch.float32) # output sample\n",
    "optimizer = torch.optim.NAdam([w], lr = 0.01)\n",
    "print(w)\n",
    "for _ in range(1000):\n",
    "    y_pred = x @ w\n",
    "    mse = torch.mean(torch.square(y - y_pred))\n",
    "    optimizer.zero_grad()\n",
    "    mse.backward()\n",
    "    optimizer.step()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cdb25-b8bc-41e2-b8b7-83e5af3bc280",
   "metadata": {},
   "source": [
    "<font color='#808080'>\n",
    "\n",
    "What the above code does is the following:\n",
    "1. First, it creates a variable vector $w$ of $3$ values, (the coefficients $A,B,C$).\n",
    "2. you create an array of shape $(N, 3)$,($N$ is the number of samples in the array $X$). This array has 3 columns: the values of $x^{2}$, $x$, and $1$,respectively(using the __np.hstack()__).\n",
    "3. you build the tensor $y$ from the NumPy array $Y$.\n",
    "4. you use a for loop to run the gradient descent in 1,000 iterations. In each\n",
    "iteration, you compute $x \\times w$ in matrix form to find $Ax^2 + Bx + C$ and assign it to the\n",
    "variable $y_{pred}$. Then, compare $y$ and $y_{pred}$ and find the mean square error.\n",
    "5. derive the gradient using the __backward()__ function. And based on this gradient, you use gradient descent to update w via the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b602cb7-91f6-4cdf-b1f4-eba98b3343be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6290],\n",
      "        [-0.1884],\n",
      "        [-0.3679]], requires_grad=True)\n",
      "tensor([[1.0173],\n",
      "        [1.9345],\n",
      "        [1.7159]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "polynomial = np.poly1d([1, 2, 3])\n",
    "N = 20 # number of samples\n",
    "# Generate random samples roughly between -10 to +10\n",
    "X = np.random.randn(N,1) * 5\n",
    "Y = polynomial(X)\n",
    "# Prepare input as an array of shape (N,3)\n",
    "XX = np.hstack([X*X, X, np.ones_like(X)])\n",
    "# Prepare tensors\n",
    "w = torch.randn(3, 1, requires_grad = True) # the 3 coefficients\n",
    "x = torch.tensor(XX, dtype = torch.float32) # input sample\n",
    "y = torch.tensor(Y, dtype = torch.float32) # output sample\n",
    "optimizer = torch.optim.NAdam([w], lr = 0.01)\n",
    "print(w)\n",
    "# Run optimizer\n",
    "for _ in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = x @ w\n",
    "    mse = torch.mean(torch.square(y - y_pred))\n",
    "    mse.backward()\n",
    "    optimizer.step()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9697339-2171-457e-9949-74e1088c99f7",
   "metadata": {},
   "source": [
    "# <h2 style=\"color: blue;\"> 3.Using Autograd to Solve a Math Puzzle <a id='AutogradSolvePuzzle'></a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28123c89-42c3-479d-8079-9bccbe06ad66",
   "metadata": {},
   "source": [
    "<font color='#808080'>\n",
    "\n",
    "You may use gradient descent to solve some math puzzles as well. For example, the following\n",
    "problem:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "   A & + & B & = & 9\\\\\n",
    "   \\underset{+}{} & & \\underset{-}{} \\\\\n",
    "   C & - & D & = & 1 \\\\\n",
    "   \\underset{=}{} & & \\underset{=}{} \\\\\n",
    "   8 &   & 2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In other words, to find the values of $A,B,C,D$ such that:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "  A + B & = 9 \\\\\n",
    "  C - D & = 1 \\\\\n",
    "  A + C & = 8 \\\\\n",
    "  B - D & = 2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This can also be solved using autograd, as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c4fde3-1edd-458a-8c57-34e3ac425f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5062, requires_grad=True)\n",
      "tensor(4.4938, requires_grad=True)\n",
      "tensor(3.4938, requires_grad=True)\n",
      "tensor(2.4938, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "A = torch.tensor(random.random(), requires_grad = True)\n",
    "B = torch.tensor(random.random(), requires_grad = True)\n",
    "C = torch.tensor(random.random(), requires_grad = True)\n",
    "D = torch.tensor(random.random(), requires_grad = True)\n",
    "# Gradient descent loop\n",
    "EPOCHS = 2000\n",
    "optimizer = torch.optim.NAdam([A, B, C, D], lr = 0.01)\n",
    "for _ in range(EPOCHS):\n",
    "    y1 = A + B - 9\n",
    "    y2 = C - D - 1\n",
    "    y3 = A + C - 8\n",
    "    y4 = B - D - 2\n",
    "    sqerr = y1*y1 + y2*y2 + y3*y3 + y4*y4\n",
    "    optimizer.zero_grad()\n",
    "    sqerr.backward()\n",
    "    optimizer.step()\n",
    "print(A)\n",
    "print(B)\n",
    "print(C)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983c1e9-c7a1-4efd-88fe-19b30c92607f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
